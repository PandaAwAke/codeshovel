diff --git a/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java b/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java
index a4b94b007fd..0b34a95710c 100644
--- a/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java
+++ b/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java
@@ -116,162 +116,164 @@ public abstract class BlendedTermQuery extends Query {
             if (minSumTTF != -1 && ctx.totalTermFreq() != -1) {
                 // we need to find out the minimum sumTTF to adjust the statistics
                 // otherwise the statistics don't match
                 minSumTTF = Math.min(minSumTTF, reader.getSumTotalTermFreq(terms[i].field()));
             } else {
                 minSumTTF = -1;
             }
 
         }
         if (minSumTTF != -1 && maxDoc > minSumTTF) {
             maxDoc = (int)minSumTTF;
         }
 
         if (max == 0) {
             return; // we are done that term doesn't exist at all
         }
         long sumTTF = minSumTTF == -1 ? -1 : 0;
         final int[] tieBreak = new int[contexts.length];
         for (int i = 0; i < tieBreak.length; ++i) {
             tieBreak[i] = i;
         }
         new InPlaceMergeSorter() {
             @Override
             protected void swap(int i, int j) {
                 final int tmp = tieBreak[i];
                 tieBreak[i] = tieBreak[j];
                 tieBreak[j] = tmp;
             }
             @Override
             protected int compare(int i, int j) {
                 return Integer.compare(contexts[tieBreak[j]].docFreq(), contexts[tieBreak[i]].docFreq());
             }
         }.sort(0, tieBreak.length);
         int prev = contexts[tieBreak[0]].docFreq();
         int actualDf = Math.min(maxDoc, max);
         assert actualDf >=0 : "DF must be >= 0";
 
 
         // here we try to add a little bias towards
         // the more popular (more frequent) fields
         // that acts as a tie breaker
         for (int i : tieBreak) {
             TermContext ctx = contexts[i];
             if (ctx.docFreq() == 0) {
                 break;
             }
             final int current = ctx.docFreq();
             if (prev > current) {
                 actualDf++;
             }
-            contexts[i] = ctx = adjustDF(ctx, Math.min(maxDoc, actualDf));
+            contexts[i] = ctx = adjustDF(reader.getContext(), ctx, Math.min(maxDoc, actualDf));
             prev = current;
             if (sumTTF >= 0 && ctx.totalTermFreq() >= 0) {
                 sumTTF += ctx.totalTermFreq();
             } else {
                 sumTTF = -1;  // omit once TF is omitted anywhere!
             }
         }
         sumTTF = Math.min(sumTTF, minSumTTF);
         for (int i = 0; i < contexts.length; i++) {
             int df = contexts[i].docFreq();
             if (df == 0) {
                 continue;
             }
             // the blended sumTTF can't be greater than the sumTTTF on the field
             final long fixedTTF = sumTTF == -1 ? -1 : sumTTF;
-            contexts[i] = adjustTTF(contexts[i], fixedTTF);
+            contexts[i] = adjustTTF(reader.getContext(), contexts[i], fixedTTF);
         }
     }
 
-    private TermContext adjustTTF(TermContext termContext, long sumTTF) {
+    private TermContext adjustTTF(IndexReaderContext readerContext, TermContext termContext, long sumTTF) {
+        assert termContext.wasBuiltFor(readerContext);
         if (sumTTF == -1 && termContext.totalTermFreq() == -1) {
             return termContext;
         }
-        TermContext newTermContext = new TermContext(termContext.topReaderContext);
-        List<LeafReaderContext> leaves = termContext.topReaderContext.leaves();
+        TermContext newTermContext = new TermContext(readerContext);
+        List<LeafReaderContext> leaves = readerContext.leaves();
         final int len;
         if (leaves == null) {
             len = 1;
         } else {
             len = leaves.size();
         }
         int df = termContext.docFreq();
         long ttf = sumTTF;
         for (int i = 0; i < len; i++) {
             TermState termState = termContext.get(i);
             if (termState == null) {
                 continue;
             }
             newTermContext.register(termState, i, df, ttf);
             df = 0;
             ttf = 0;
         }
         return newTermContext;
     }
 
-    private static TermContext adjustDF(TermContext ctx, int newDocFreq) {
+    private static TermContext adjustDF(IndexReaderContext readerContext, TermContext ctx, int newDocFreq) {
+        assert ctx.wasBuiltFor(readerContext);
         // Use a value of ttf that is consistent with the doc freq (ie. gte)
         long newTTF;
         if (ctx.totalTermFreq() < 0) {
             newTTF = -1;
         } else {
             newTTF = Math.max(ctx.totalTermFreq(), newDocFreq);
         }
-        List<LeafReaderContext> leaves = ctx.topReaderContext.leaves();
+        List<LeafReaderContext> leaves = readerContext.leaves();
         final int len;
         if (leaves == null) {
             len = 1;
         } else {
             len = leaves.size();
         }
-        TermContext newCtx = new TermContext(ctx.topReaderContext);
+        TermContext newCtx = new TermContext(readerContext);
         for (int i = 0; i < len; ++i) {
             TermState termState = ctx.get(i);
             if (termState == null) {
                 continue;
             }
             newCtx.register(termState, i, newDocFreq, newTTF);
             newDocFreq = 0;
             newTTF = 0;
         }
         return newCtx;
     }
 
     public List<Term> getTerms() {
         return Arrays.asList(terms);
     }
 
     @Override
     public String toString(String field) {
         StringBuilder builder = new StringBuilder("blended(terms:[");
         for (int i = 0; i < terms.length; ++i) {
             builder.append(terms[i]);
             float boost = 1f;
             if (boosts != null) {
                 boost = boosts[i];
             }
             if (boost != 1f) {
                 builder.append('^').append(boost);
             }
             builder.append(", ");
         }
         if (terms.length > 0) {
             builder.setLength(builder.length() - 2);
         }
         builder.append("])");
         return builder.toString();
     }
 
     private volatile Term[] equalTerms = null;
 
     private Term[] equalsTerms() {
         if (terms.length == 1) {
             return terms;
         }
         if (equalTerms == null) {
             // sort the terms to make sure equals and hashCode are consistent
             // this should be a very small cost and equivalent to a HashSet but less object creation
             final Term[] t = new Term[terms.length];
             System.arraycopy(terms, 0, t, 0, terms.length);
             ArrayUtil.timSort(t);
             equalTerms = t;
